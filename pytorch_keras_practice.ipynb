{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch-keras-practice.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e0fab513b6ec46bfb3de903307fa7a7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ae53b10c355e4dccbcdce87b795b9209",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_03aed6a152b7419fb6edc9925b2fa7f2",
              "IPY_MODEL_6fb024bd72b94adfba609d36a2b9b8c5"
            ]
          }
        },
        "ae53b10c355e4dccbcdce87b795b9209": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "03aed6a152b7419fb6edc9925b2fa7f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3c0b49dc3db44b7aa345cb1dfcc7ae27",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 87306240,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 87306240,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e2108026dc46487b94ef924af6c1629b"
          }
        },
        "6fb024bd72b94adfba609d36a2b9b8c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5056505173a64b9383fe3530f4df60b5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 83.3M/83.3M [00:00&lt;00:00, 117MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b999895aae434273abce690d07dd8406"
          }
        },
        "3c0b49dc3db44b7aa345cb1dfcc7ae27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e2108026dc46487b94ef924af6c1629b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5056505173a64b9383fe3530f4df60b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b999895aae434273abce690d07dd8406": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LL-Jan/Kaggle/blob/main/pytorch_keras_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9J4JxSL6flE",
        "outputId": "7d8adf3a-f5db-43b3-954e-15ad11c760e3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgQf1mfi6flG",
        "scrolled": true
      },
      "source": [
        "!pip install segmentation-models-pytorch==0.1.3\n",
        "!pip install albumentations==0.5.2 \n",
        "!pip install netron\n",
        "!pip install plotly==4.14.3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVCKXF5E6flH"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import os\n",
        "import gc\n",
        "import json \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# pd.set_option(\"display.max_rows\", 101)\n",
        "# pd.set_option(\"expand_frame_repr\", True)\n",
        "# pd.set_option(\"mode.use_inf_as_na\", True)\n",
        "# pd.options.plotting.backend = 'plotly'\n",
        "\n",
        "from random import shuffle\n",
        "\n",
        "import time\n",
        "from datetime import datetime as dt\n",
        "from pytz import timezone\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# plt.style.use('dark_background')\n",
        "# %matplotlib widget\n",
        "%matplotlib inline\n",
        "\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go \n",
        "# template = 'plotly_dark'\n",
        "template = 'plotly'\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import cv2 as cv\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset \n",
        "from torchvision.datasets import ImageFolder \n",
        "from torchvision import transforms as tfs \n",
        "import torchvision.models as models \n",
        "\n",
        "import albumentations as amt\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "import segmentation_models_pytorch as smp\n",
        "import netron"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPLBJE9a8pL3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df63b240-575d-4721-e7d1-49a6a5da5a38"
      },
      "source": [
        "torch.cuda.is_available(), torch.cuda.get_device_name(0)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(True, 'Tesla T4')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_Vu4ax06flI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b6eebdf-aa38-4c68-e139-ee8bceef150c"
      },
      "source": [
        "random_state = 618\n",
        "torch.manual_seed(random_state)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f435d2acc90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNP38BZv6flI"
      },
      "source": [
        "# EDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fj7E1C3yF93c"
      },
      "source": [
        "# Google Colab\n",
        "nb_path = \"/content/drive/MyDrive/Colab/Kaggle/severstal-steel-defect-detection\"\n",
        "out_path = np_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vP-w45TaGXeV"
      },
      "source": [
        "# # Kaggle\n",
        "# nb_path = \"../input/severstal-steel-defect-detection\"\n",
        "# out_path = \"./\""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YneVX2uf6flI"
      },
      "source": [
        "train = pd.read_csv(os.path.join(nb_path, \"train.csv\"))\n",
        "# train = pd.read_csv(\"./severstal-steel-defect-detection/train.csv\")                "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrRnPMYY6flJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "73f9e5ea-7180-46ac-a375-7a5c6a0c0a37"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ImageId</th>\n",
              "      <th>ClassId</th>\n",
              "      <th>EncodedPixels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0002cc93b.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>29102 12 29346 24 29602 24 29858 24 30114 24 3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0007a71bf.jpg</td>\n",
              "      <td>3</td>\n",
              "      <td>18661 28 18863 82 19091 110 19347 110 19603 11...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>000a4bcdd.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>37607 3 37858 8 38108 14 38359 20 38610 25 388...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>000f6bf48.jpg</td>\n",
              "      <td>4</td>\n",
              "      <td>131973 1 132228 4 132483 6 132738 8 132993 11 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0014fce06.jpg</td>\n",
              "      <td>3</td>\n",
              "      <td>229501 11 229741 33 229981 55 230221 77 230468...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         ImageId  ClassId                                      EncodedPixels\n",
              "0  0002cc93b.jpg        1  29102 12 29346 24 29602 24 29858 24 30114 24 3...\n",
              "1  0007a71bf.jpg        3  18661 28 18863 82 19091 110 19347 110 19603 11...\n",
              "2  000a4bcdd.jpg        1  37607 3 37858 8 38108 14 38359 20 38610 25 388...\n",
              "3  000f6bf48.jpg        4  131973 1 132228 4 132483 6 132738 8 132993 11 ...\n",
              "4  0014fce06.jpg        3  229501 11 229741 33 229981 55 230221 77 230468..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aR3Ztnaf6flJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62ad44fa-ab1a-426c-fbd7-576e0eb305f2"
      },
      "source": [
        "train.info()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 7095 entries, 0 to 7094\n",
            "Data columns (total 3 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   ImageId        7095 non-null   object\n",
            " 1   ClassId        7095 non-null   int64 \n",
            " 2   EncodedPixels  7095 non-null   object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 166.4+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWiBDOgv6flJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13b5253e-fcbd-4693-91b1-efaf950f7517"
      },
      "source": [
        "train['ImageId'].nunique(), train['EncodedPixels'].nunique()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6666, 7095)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HJtmm8FOM-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b3cc539-86b0-4bcb-fa42-9fd94003e77b"
      },
      "source": [
        "train['ClassId'].nunique(), train['ClassId'].unique()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, array([1, 3, 4, 2]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cCpl1vH6flK",
        "outputId": "f67f1e0f-32c5-4942-b9e4-d74e6c2840a4"
      },
      "source": [
        "train_image_path = os.path.join(nb_path, \"train_images\")\n",
        "test_image_path = os.path.join(nb_path, \"test_images\")\n",
        "\n",
        "# train_image_path = \"./severstal-steel-defect-detection/train_images/\"\n",
        "# test_image_path = \"./severstal-steel-defect-detection/test_images/\"\n",
        "\n",
        "print(f\"{len(os.listdir(train_image_path))} images in training set\")\n",
        "print(f\"{len(os.listdir(test_image_path))} images in test set\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13011 images in training set\n",
            "5702 images in test set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4ks2mWs6flK"
      },
      "source": [
        "# Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-lt8Ute6flK"
      },
      "source": [
        "## Alignment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gssKpmIc6flK"
      },
      "source": [
        "# Images without defects are not included in the train.csv, we should concat them.\n",
        "df = train.pivot(index='ImageId', columns='ClassId', values='EncodedPixels')\n",
        "df = df.merge(pd.DataFrame(index=os.listdir(train_image_path)), \n",
        "              left_index=True, \n",
        "              right_index=True, \n",
        "              how='right', \n",
        "              validate='one_to_one')\n",
        "\n",
        "df['num_defect'] = df.count(axis=1)\n",
        "df['num_defect'] = df['num_defect'].astype(np.uint8)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "_CkK7TtD6flL",
        "outputId": "8bca68c2-6277-40db-9637-ccfcfce80b50"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>num_defect</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ea970bedf.jpg</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>63939 1 64195 2 64451 5 64706 9 64962 11 65218...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>eb4225311.jpg</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>303400 7 303654 15 303908 19 304163 19 304187 ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ec0e3a1c2.jpg</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>148340 2 148594 4 148848 6 149102 8 149357 9 1...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>eaf7443a7.jpg</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>70658 255 70914 255 71170 255 71426 255 71682 ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ea7752e39.jpg</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 1    2  ...    4 num_defect\n",
              "ea970bedf.jpg  NaN  NaN  ...  NaN          1\n",
              "eb4225311.jpg  NaN  NaN  ...  NaN          1\n",
              "ec0e3a1c2.jpg  NaN  NaN  ...  NaN          1\n",
              "eaf7443a7.jpg  NaN  NaN  ...  NaN          1\n",
              "ea7752e39.jpg  NaN  NaN  ...  NaN          0\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mg3nIlqr6flL",
        "outputId": "86075a3f-c46b-4118-a83e-2529d5146254"
      },
      "source": [
        "df['num_defect'].value_counts().sort_index()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    6345\n",
              "1    6239\n",
              "2     425\n",
              "3       2\n",
              "Name: num_defect, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ez78PNE06flL"
      },
      "source": [
        "## Split to training/validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IckzYMJF6flL",
        "outputId": "ceff1205-c9a4-45c7-d606-c60cbc14dcf3"
      },
      "source": [
        "train_df, valid_df = train_test_split(df, \n",
        "                                      test_size=0.2, \n",
        "                                      random_state=random_state, \n",
        "                                      shuffle=True, \n",
        "                                      stratify=df['num_defect'])\n",
        "train_df = train_df.drop(columns=['num_defect'])\n",
        "valid_df = valid_df.drop(columns=['num_defect'])\n",
        "train_df.shape, valid_df.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((10408, 4), (2603, 4))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OO4jySF-6flM"
      },
      "source": [
        "train_id = [] # List of image id like 'xxx.jpg' for training set\n",
        "train_id.extend(train_df.index.to_list())\n",
        "train_id.extend([s.split('.')[0]+'_HF.jpg' for s in train_df.index]) # Add image id for HorizontalFlip\n",
        "train_id.extend([s.split('.')[0]+'_VF.jpg' for s in train_df.index]) # Add image id for VerticalFlip\n",
        "train_id.extend([s.split('.')[0]+'_HVF.jpg' for s in train_df.index]) # Add image id for HorizontalFlip and VerticalFlip\n",
        "\n",
        "# train_id = train_id[:16]\n",
        "valid_id = valid_df.index.to_list() # List of image id like 'xxx.jpg' for validation set"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KnxJQwt6flM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1afec17-4581-4ae6-86e7-1fce1a706843"
      },
      "source": [
        "len(train_id), len(valid_id)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(41632, 2603)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gg8vbvve6flM"
      },
      "source": [
        "shuffle(train_id)\n",
        "# train_id[0]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqeOUASt6flN"
      },
      "source": [
        "def get_img_id(idx, img_id_list):\n",
        "    img_id = img_id_list[idx]\n",
        "    if '_' not in img_id:\n",
        "        img_id, augment = img_id, None\n",
        "    elif '_HF' in img_id: \n",
        "        img_id, augment = img_id.replace('_HF',''), 'HF'\n",
        "    elif '_VF' in img_id: \n",
        "        img_id, augment = img_id.replace('_VF',''), 'VF'\n",
        "    elif '_HVF' in img_id: \n",
        "        img_id, augment = img_id.replace('_HVF',''), 'HVF'\n",
        "    return img_id, augment"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QBCfIxT6flN"
      },
      "source": [
        "## Image Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LG8gD-qS6flN"
      },
      "source": [
        "# Define the pipeline for augmentation, Normalizing and totensor transform\n",
        "def take_trfm(mean, std, augment):\n",
        "    if not augment: \n",
        "        trfms = amt.Compose([amt.Normalize(mean=mean, std=std, p=1), \n",
        "                             ToTensorV2(transpose_mask=True)])\n",
        "    elif augment == 'HF': \n",
        "        trfms = amt.Compose([amt.HorizontalFlip(p=1), \n",
        "                             amt.Normalize(mean=mean, std=std, p=1), \n",
        "                             ToTensorV2(transpose_mask=True)])\n",
        "    elif augment == 'VF': \n",
        "        trfms = amt.Compose([amt.VerticalFlip(p=1), \n",
        "                             amt.Normalize(mean=mean, std=std, p=1), \n",
        "                             ToTensorV2(transpose_mask=True)])\n",
        "    elif augment == 'HVF': \n",
        "        trfms = amt.Compose([amt.HorizontalFlip(p=1), \n",
        "                             amt.VerticalFlip(p=1), \n",
        "                             amt.Normalize(mean=mean, std=std, p=1), \n",
        "                             ToTensorV2(transpose_mask=True)])\n",
        "    return trfms"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtVRhDQF6flN"
      },
      "source": [
        "## Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRdu6tZh6flN"
      },
      "source": [
        "def get_mask(img_id, df):\n",
        "    \n",
        "    mask = np.zeros((256, 1600, 4))\n",
        "    defects = [] \n",
        "    \n",
        "    for i, label in enumerate(df.loc[img_id,:].to_list()):\n",
        "        if label is not np.nan: \n",
        "            label = [int(x) for x in label.split()]\n",
        "            pix_starts, pix_lengths = label[::2], label[1::2]\n",
        "            mask_ = np.zeros((256*1600, 1))\n",
        "            for start, length in zip(pix_starts, pix_lengths):\n",
        "                mask_[start:(start+length)] = 1\n",
        "            mask[:,:,i] = mask_.reshape((256, 1600), order='F') \n",
        "            defects.append(i+1)\n",
        "            \n",
        "    return mask"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arHQb6p06flQ"
      },
      "source": [
        "# DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMHIbSGG6flR"
      },
      "source": [
        "## Map-style Dataset â†’ DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "algDjjAN6flT"
      },
      "source": [
        "# Inherit torch.utils.data.Dataset and override '__getitem__' and '__len__'\n",
        "# Preprocessing \n",
        "class SteelDataset(Dataset):\n",
        "    def __init__(self, \n",
        "                 df, \n",
        "                 image_path, \n",
        "                 img_id_list, \n",
        "                 mean=(0.485, 0.456, 0.406), \n",
        "                 std=(0.229, 0.224, 0.225)):\n",
        "        self.df = df\n",
        "        self.image_path = image_path \n",
        "        self.img_id_list = img_id_list\n",
        "        self.mean = mean \n",
        "        self.std = std\n",
        "    \n",
        "    def __getitem__(self, idx): \n",
        "        img_id, augment = get_img_id(idx, self.img_id_list)\n",
        "        img_path = os.path.join(self.image_path, img_id)\n",
        "        img = cv.imread(img_path)\n",
        "        mask = get_mask(img_id, self.df)\n",
        "        trfm = take_trfm(self.mean, self.std, augment)\n",
        "        amted = trfm(image=img, mask=mask)\n",
        "        img, mask = amted['image'], amted['mask'] \n",
        "        return img, mask\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.img_id_list)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSJCu0M-6flT"
      },
      "source": [
        "train_dataset = SteelDataset(train_df, \n",
        "                             image_path=train_image_path, \n",
        "                             img_id_list=train_id)\n",
        "valid_dataset = SteelDataset(valid_df, \n",
        "                             image_path=train_image_path, \n",
        "                             img_id_list=valid_id)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCLJMV9_6flU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a4daa16-b8db-49da-d245-7468d4104b1b"
      },
      "source": [
        "len(train_dataset), len(valid_dataset)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(41632, 2603)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CygRLrIz6flU"
      },
      "source": [
        "batch_size = 8\n",
        "train_dataloader = DataLoader(train_dataset, \n",
        "                              batch_size=batch_size, \n",
        "                              shuffle=True)\n",
        "valid_dataloader = DataLoader(valid_dataset, \n",
        "                              batch_size=batch_size, \n",
        "                              shuffle=True)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOxSOTHa6flU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "981f2ecd-e2b3-48a7-bf96-d2d0dd63b168"
      },
      "source": [
        "len(train_dataloader), len(valid_dataloader)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5204, 326)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kv4UHvcb6flU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cd04e2b-aa66-41cb-9ddf-6588b9f5c183"
      },
      "source": [
        "for batch, (X, y) in enumerate(train_dataloader): \n",
        "    print(batch, X.shape, y.shape)\n",
        "    break"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 torch.Size([8, 3, 256, 1600]) torch.Size([8, 4, 256, 1600])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIVhWcKk6flV"
      },
      "source": [
        "## TensorData â†’ DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLA1PCRo6flV"
      },
      "source": [
        ""
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj_Qdn3R6flV"
      },
      "source": [
        "## ImageFolder â†’ DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkA4xPCy6flV"
      },
      "source": [
        ""
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXkjhGRz6flV"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3ZHWioj6flV"
      },
      "source": [
        "# Sum of dice in a batch\n",
        "def cal_dice(output, mask_gt, threshold=0.5): \n",
        "    '''Calculate the dice of a batch (between ground truth mask and output tensor)'''\n",
        "    \n",
        "    batch_size = len(mask_gt)\n",
        "    prob = torch.sigmoid(output)\n",
        "#     mask_pred = (prob > threshold).astype('torch.uint8')\n",
        "    mask_pred = (prob > threshold).int()\n",
        "    \n",
        "    assert mask_gt.shape == mask_pred.shape \n",
        "    \n",
        "    mask_gt = mask_gt.reshape(shape=(batch_size, -1))\n",
        "    mask_pred = mask_pred.reshape(shape=(batch_size, -1))\n",
        "    \n",
        "    idx_neg = torch.nonzero(mask_gt.sum(-1)==0)\n",
        "    idx_pos = torch.nonzero(mask_gt.sum(-1)>=1)\n",
        "    \n",
        "    dice_neg = (mask_pred.sum(-1)==0).float()\n",
        "    dice_pos = 2 * (mask_gt * mask_pred).sum(-1) / (mask_gt + mask_pred).sum(-1)\n",
        "    \n",
        "    dice = torch.cat([dice_pos[idx_pos], dice_neg[idx_neg]])\n",
        "        \n",
        "#     dice = dice.numpy() # torch.tensor to numpy.ndarray\n",
        "#     dice = np.nanmean(dice) # Calculate the mean dice of a batch, ignore nan\n",
        "    \n",
        "    return dice.sum()"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9jCp5DY6flW"
      },
      "source": [
        "# Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCLyTjj66flW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b10fafbd-da59-4ced-9587-8a121db16b1b"
      },
      "source": [
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5xm_RmS6flW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "e0fab513b6ec46bfb3de903307fa7a7a",
            "ae53b10c355e4dccbcdce87b795b9209",
            "03aed6a152b7419fb6edc9925b2fa7f2",
            "6fb024bd72b94adfba609d36a2b9b8c5",
            "3c0b49dc3db44b7aa345cb1dfcc7ae27",
            "e2108026dc46487b94ef924af6c1629b",
            "5056505173a64b9383fe3530f4df60b5",
            "b999895aae434273abce690d07dd8406"
          ]
        },
        "outputId": "a75295ec-e117-4b04-89f8-4be30344f26c"
      },
      "source": [
        "# state_file = \"../input/scores/model_state.pth\"\n",
        "# state_file = os.path.join(nb_path, \"model_state.pth\")\n",
        "# model = smp.Unet(encoder_name='resnet34', \n",
        "#                  encoder_weights='imagenet', \n",
        "#                  in_channels=3, \n",
        "#                  classes=4,)\n",
        "# if os.path.isfile(state_file):\n",
        "#     model.load_state_dict(torch.load(state_file))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e0fab513b6ec46bfb3de903307fa7a7a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=87306240.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOeOppji6flX"
      },
      "source": [
        "## Visiualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghO9rw4j6flX"
      },
      "source": [
        "# model_path = 'C:/Users/ll/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth'\n",
        "# model_path = 'C:/Users/ll/.cache/torch/hub/checkpoints/deeplabv3_resnet101_coco-586e9e4e.pth'\n",
        "\n",
        "# torch.onnx.export(model, torch.rand(8, 3, 256, 1600), 'onnx_model.onnx') \n",
        "# torch.onnx.export(model_, torch.rand(8, 3, 256, 1600), 'onnx_model_.onnx')\n",
        "\n",
        "# netron.start('onnx_model.onnx')"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7IA0kdU6flY"
      },
      "source": [
        "# Train & Validate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUljdjFr6flY"
      },
      "source": [
        "def train(dataloader, model, loss_fn, optimizer, batch_size=batch_size):\n",
        "    t0 = time.time()\n",
        "    size = len(dataloader.dataset) # Total number of images in dataset\n",
        "    n_batch = len(dataloader)\n",
        "    cum_loss, cum_dice = 0, 0\n",
        "\n",
        "    model.train() # Very important\n",
        "\n",
        "    for batch, (X, y) in enumerate(dataloader): \n",
        "        X, y = X.to(device), y.to(device)\n",
        "        \n",
        "        output = model(X)\n",
        "        loss = loss_fn(output, y)\n",
        "#         output = output.detach().cpu()\n",
        "#         y = y.detach().cpu()\n",
        "        dice = cal_dice(output, y)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        cum_loss = cum_loss + loss.item()\n",
        "        cum_dice = cum_dice + dice.item()\n",
        "        \n",
        "        if batch % 300 == 0:\n",
        "            t1 = time.time()\n",
        "            print(f'Training Batch: {batch}/{n_batch}. Loss: {loss.item():.4f}. Dice: {dice.item()/batch_size:.4f}. Cost: {int(t1-t0)}s') \n",
        "    \n",
        "    return cum_loss/size, cum_dice/size"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVk3dhGW6flY"
      },
      "source": [
        "def validate(dataloader, model, loss_fn, batch_size=batch_size):\n",
        "    t0 = time.time()\n",
        "    size = len(dataloader.dataset)\n",
        "    n_batch = len(dataloader)\n",
        "    cum_loss, cum_dice = 0, 0\n",
        "\n",
        "    model.eval() # Very important\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch, (X, y) in enumerate(dataloader): \n",
        "            X, y = X.to(device), y.to(device)\n",
        "        \n",
        "            output = model(X)\n",
        "            loss = loss_fn(output, y)\n",
        "#             output = output.detach().cpu()\n",
        "#             y = y.detach().cpu()\n",
        "            dice = cal_dice(output, y) \n",
        "            \n",
        "            cum_loss = cum_loss + loss.item()\n",
        "            cum_dice = cum_dice + dice.item()\n",
        "\n",
        "            if batch % 30 == 0:\n",
        "                t1 = time.time()\n",
        "                print(f'Validation Batch: {batch}/{n_batch}. Loss: {loss.item():.4f}. Dice: {dice.item()/batch_size:.4f}. Cost: {int(t1-t0)}s') \n",
        "                \n",
        "    return cum_loss/size, cum_dice/size"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKWSrFgL6flY"
      },
      "source": [
        "score_file = os.path.join(nb_path, \"scores.csv\")\n",
        "if os.path.isfile(score_file): \n",
        "    score_df = pd.read_csv(score_file, index_col=['epoch'])\n",
        "else:\n",
        "    score_df = pd.DataFrame(index=range(1,21), \n",
        "                            columns=['train_loss', 'train_dice', 'valid_loss', 'valid_dice'], \n",
        "                            dtype='float32')\n",
        "    score_df = score_df.rename_axis(mapper='epoch', axis='index')\n",
        "epoch_untrained = score_df[score_df['train_loss'].isna()].index.to_list()"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8SOxMWL5u3D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c8c6bc33-ef55-4ed2-d0a7-bd63e0ccb4f5"
      },
      "source": [
        "f\"{min(epoch_untrained)-1} Epochs Trained.\""
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'0 Epochs Trained.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__IeZmH56flZ",
        "outputId": "369400dc-0166-4df6-bf5b-f6c5bb0da088"
      },
      "source": [
        "for t in tqdm(epoch_untrained[:1]):\n",
        "    \n",
        "    print(f\"Epoch {t} starts at {dt.now(tz=timezone('Asia/Shanghai')).strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    t0 = time.time()\n",
        "\n",
        "    # model_file = f\"../input/scores/model_trained_{t-1}.pth\"\n",
        "    model_file = os.path.join(nb_path, f\"model_trained_{t-1}.pth\")\n",
        "    if os.path.isfile(model_file): \n",
        "        model = torch.load(model_file)\n",
        "        print(f\"Use existed trained model.\")\n",
        "    else: \n",
        "        model = smp.Unet(encoder_name='resnet34', \n",
        "                         encoder_weights='imagenet', \n",
        "                         in_channels=3, \n",
        "                         classes=4,)\n",
        "        print(f\"Use pretrained model.\")\n",
        "\n",
        "    model = model.to(device) # Very important\n",
        "\n",
        "    loss = nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "    train_loss, train_dice = train(train_dataloader, \n",
        "                                   model=model, \n",
        "                                   loss_fn=loss, \n",
        "                                   optimizer=optimizer) \n",
        "    valid_loss, valid_dice = validate(valid_dataloader, \n",
        "                                      model=model, \n",
        "                                      loss_fn=loss) \n",
        "    \n",
        "    score_df.loc[t,:] = [train_loss,train_dice,train_loss,valid_dice]\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    torch.save(model, os.path.join(out_path, f\"model_trained_{t}.pth\")) \n",
        "    print(f\"Epoch {t} Model Saved!\")\n",
        "    torch.save(model.state_dict(), os.path.join(out_path, f\"model_state_{t}.pth\"))\n",
        "    print(f\"Epoch {t} Model State Saved!\")\n",
        "    score_df.to_csv(os.path.join(out_path, f\"scores.csv\"))\n",
        "    score_df.to_csv(os.path.join(out_path, f\"scores_{t}.csv\"))\n",
        "    print(f\"Epoch {t} Scores Saved!\")\n",
        "\n",
        "    t1 = time.time()\n",
        "    print(f\"Epoch {t} ends at {dt.now(tz=timezone('Asia/Shanghai')).strftime('%Y-%m-%d %H:%M:%S')}. Total Cost: {(t1-t0)/60:.2f}min\")\n",
        "\n",
        "print(\"Done!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 starts at 2021-05-24 19:03:20\n",
            "Training Batch: 0/5204. Loss: 1.0213. Dice: 0.0018. Cost: 1s\n",
            "Training Batch: 300/5204. Loss: 0.2136. Dice: 0.1250. Cost: 290s\n",
            "Training Batch: 600/5204. Loss: 0.0996. Dice: 0.3750. Cost: 587s\n",
            "Training Batch: 900/5204. Loss: 0.0791. Dice: 0.2500. Cost: 882s\n",
            "Training Batch: 1200/5204. Loss: 0.0359. Dice: 0.7500. Cost: 1178s\n",
            "Training Batch: 1500/5204. Loss: 0.0247. Dice: 0.4657. Cost: 1473s\n",
            "Training Batch: 1800/5204. Loss: 0.0210. Dice: 0.4589. Cost: 1769s\n",
            "Training Batch: 2100/5204. Loss: 0.0147. Dice: 0.8786. Cost: 2064s\n",
            "Training Batch: 2400/5204. Loss: 0.0619. Dice: 0.5120. Cost: 2359s\n",
            "Training Batch: 2700/5204. Loss: 0.0132. Dice: 0.4974. Cost: 2655s\n",
            "Training Batch: 3000/5204. Loss: 0.0078. Dice: 0.7209. Cost: 2951s\n",
            "Training Batch: 3300/5204. Loss: 0.0112. Dice: 0.7317. Cost: 3246s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}